{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkNCI/AI-Ml-Diploma/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5290c5",
      "metadata": {
        "id": "7e5290c5"
      },
      "source": [
        "# Lecture NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c75dcb70",
      "metadata": {
        "id": "c75dcb70"
      },
      "source": [
        "Spacy installation \n",
        "https://spacy.io/usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a10eb2f",
      "metadata": {
        "id": "9a10eb2f",
        "outputId": "f801783d-9017-4122-8384-e08883be2990"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 21:36:27.171749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0587cd90",
      "metadata": {
        "id": "0587cd90"
      },
      "source": [
        "### Sentence Detection: Sentence detection is the process of locating where sentences start and end in a given text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd1865bc",
      "metadata": {
        "id": "fd1865bc"
      },
      "source": [
        "In the above example, spaCy is correctly able to identify the input’s sentences. With .sents, you get a list of Span objects representing individual sentences. You can also slice the Span objects to produce sections of a sentence. In spaCy, the .sents property is used to extract sentences from the Doc object. Here’s how you would extract the total number of sentences and the sentences themselves for a given input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c1932b",
      "metadata": {
        "id": "e8c1932b",
        "outputId": "3fd0302f-ee71-4efb-ad80-6a991717a680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "Gus Proto is a Python...\n",
            "He is interested in learning...\n"
          ]
        }
      ],
      "source": [
        "about_text = (\n",
        "        \"Gus Proto is a Python developer currently\"\n",
        "         \" working for a London-based Fintech\"\n",
        "         \" company. He is interested in learning\"\n",
        "         \" Natural Language Processing.\"\n",
        "     )\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents)\n",
        "print(len(sentences))\n",
        "\n",
        "for sentence in sentences:\n",
        "     print(f\"{sentence[:5]}...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b63d42fa",
      "metadata": {
        "id": "b63d42fa"
      },
      "source": [
        "You can also customize sentence detection behavior by using custom delimiters. Here’s an example where an ellipsis (...) is used as a delimiter, in addition to the full stop, or period (.).\n",
        "For this example, you use the @Language.component(\"set_custom_boundaries\") decorator to define a new function that takes a Doc object as an argument. The job of this function is to identify tokens in Doc that are the beginning of sentences and mark their .is_sent_start attribute to True. Once done, the function must return the Doc object again.\n",
        "\n",
        "Then, you can add the custom boundary function to the Language object by using the .add_pipe() method. Parsing text with this modified Language object will now treat the word after an ellipse as the start of a new sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a260f613",
      "metadata": {
        "id": "a260f613",
        "outputId": "b54cec26-a04c-4643-9af1-17ddfaacff86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus, can you, ...\n",
            "never mind, I forgot what I was saying.\n",
            "So, do you think we should ...\n"
          ]
        }
      ],
      "source": [
        "ellipsis_text = (\n",
        "    \"Gus, can you, ... never mind, I forgot\"\n",
        "   \" what I was saying. So, do you think\"\n",
        "   \" we should ...\"\n",
        " )\n",
        "\n",
        "from spacy.language import Language\n",
        "@Language.component(\"set_custom_boundaries\")\n",
        "def set_custom_boundaries(doc):\n",
        "     \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\"\n",
        "     for token in doc[:-1]:\n",
        "         if token.text == \"...\":\n",
        "             doc[token.i + 1].is_sent_start = True\n",
        "     return doc\n",
        "\n",
        "\n",
        "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
        "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
        "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
        "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
        "for sentence in custom_ellipsis_sentences:\n",
        " print(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7604db36",
      "metadata": {
        "id": "7604db36"
      },
      "source": [
        "### Tokens in spaCy: The process of tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as Token objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc866c8c",
      "metadata": {
        "id": "bc866c8c",
        "outputId": "5c68c68c-5cab-4f7e-e487-9480b272b739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus 0\n",
            "Proto 4\n",
            "is 10\n",
            "a 13\n",
            "Python 15\n",
            "developer 22\n",
            "currently 32\n",
            "working 42\n",
            "for 50\n",
            "a 54\n",
            "London 56\n",
            "- 62\n",
            "based 63\n",
            "Fintech 69\n",
            "company 77\n",
            ". 84\n",
            "He 86\n",
            "is 89\n",
            "interested 92\n",
            "in 103\n",
            "learning 106\n",
            "Natural 115\n",
            "Language 123\n",
            "Processing 132\n",
            ". 142\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = (\n",
        "     \"Gus Proto is a Python developer currently\"\n",
        "     \" working for a London-based Fintech\"\n",
        "     \" company. He is interested in learning\"\n",
        "     \" Natural Language Processing.\"\n",
        " )\n",
        "about_doc = nlp(about_text)\n",
        "\n",
        "for token in about_doc:\n",
        "    print (token, token.idx) #.idx attribute represents the starting position of the token in the original text. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca25ce1",
      "metadata": {
        "id": "eca25ce1"
      },
      "source": [
        "###  Stop Words: Stop words are typically defined as the most common words in a language. \n",
        "In the English language, some examples of stop words are the, are, but, and they. \n",
        "Most sentences need to contain stop words in order to be full sentences that make grammatical sense.\n",
        "\n",
        "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. spaCy stores a list of stop words for the English language:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5fe6950",
      "metadata": {
        "id": "b5fe6950",
        "outputId": "5e8c90e4-7975-4ae8-c22e-15aaba29a882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "326\n",
            "if\n",
            "regarding\n",
            "‘ve\n",
            "between\n",
            "nevertheless\n",
            "off\n",
            "hers\n",
            "everywhere\n",
            "did\n",
            "thus\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print(len(spacy_stopwords))\n",
        "\n",
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        " print(stop_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931309fc",
      "metadata": {
        "id": "931309fc",
        "outputId": "38452a28-29c0-4198-e72b-64cef2350643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
          ]
        }
      ],
      "source": [
        "custom_about_text = (\n",
        " \"Gus Proto is a Python developer currently\"\n",
        " \" working for a London-based Fintech\"\n",
        " \" company. He is interested in learning\"\n",
        " \" Natural Language Processing.\"\n",
        ")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_doc = nlp(custom_about_text)\n",
        "print([token for token in about_doc if not token.is_stop])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29cd081",
      "metadata": {
        "id": "c29cd081"
      },
      "source": [
        "### Lemmatization\n",
        "Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a lemma.\n",
        "\n",
        "For example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories, like tense (organized vs organize), number (trains vs train), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text.\n",
        "\n",
        "spaCy puts a lemma_ attribute on the Token class. This attribute has the lemmatized form of the token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5dc788",
      "metadata": {
        "id": "9f5dc788",
        "outputId": "2622c4d1-6a38-408c-ed9e-141e14d8cadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  is : be\n",
            "                  He : he\n",
            "               keeps : keep\n",
            "          organizing : organize\n",
            "             meetups : meetup\n",
            "               talks : talk\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "conference_help_text = (\n",
        " \"Gus is helping organize a developer\"\n",
        " \" conference on Applications of Natural Language\"\n",
        " \" Processing. He keeps organizing local Python meetups\"\n",
        " \" and several internal talks at his workplace.\"\n",
        ")\n",
        "conference_help_doc = nlp(conference_help_text)\n",
        "for token in conference_help_doc:\n",
        " if str(token) != str(token.lemma_):\n",
        "     print(f\"{str(token):>20} : {str(token.lemma_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9761f704",
      "metadata": {
        "id": "9761f704"
      },
      "source": [
        "### Word Frequency\n",
        "You can now convert a given text into tokens and perform statistical analysis on it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b2f923",
      "metadata": {
        "id": "a9b2f923",
        "outputId": "5ff04e3e-132f-42c5-ac49-61e75c799769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "complete_text = (\n",
        " \"Gus Proto is a Python developer currently\"\n",
        " \" working for a London-based Fintech company. He is\"\n",
        " \" interested in learning Natural Language Processing.\"\n",
        " \" There is a developer conference happening on 21 July\"\n",
        " ' 2019 in London. It is titled \"Applications of Natural'\n",
        " ' Language Processing\". There is a helpline number'\n",
        " \" available at +44-1234567891. Gus is helping organize it.\"\n",
        " \" He keeps organizing local Python meetups and several\"\n",
        " \" internal talks at his workplace. Gus is also presenting\"\n",
        " ' a talk. The talk will introduce the reader about \"Use'\n",
        " ' cases of Natural Language Processing in Fintech\".'\n",
        " \" Apart from his work, he is very passionate about music.\"\n",
        " \" Gus is learning to play the Piano. He has enrolled\"\n",
        " \" himself in the weekend batch of Great Piano Academy.\"\n",
        " \" Great Piano Academy is situated in Mayfair or the City\"\n",
        " \" of London and has world-class piano instructors.\"\n",
        ")\n",
        "complete_doc = nlp(complete_text)\n",
        "\n",
        "words = [\n",
        " token.text\n",
        " for token in complete_doc\n",
        " if not token.is_stop and not token.is_punct\n",
        "]\n",
        "\n",
        "print(Counter(words).most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8e02f8f",
      "metadata": {
        "id": "d8e02f8f"
      },
      "source": [
        "### Part-of-Speech Tagging\n",
        "Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are typically eight parts of speech:\n",
        "\n",
        "Noun,\n",
        "Pronoun,\n",
        "Adjective,\n",
        "Verb,\n",
        "Adverb,\n",
        "Preposition,\n",
        "Conjunction,\n",
        "Interjection.\n",
        "Part-of-speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\n",
        "In spaCy, POS tags are available as an attribute on the Token object:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9545ca5a",
      "metadata": {
        "id": "9545ca5a",
        "outputId": "0c0073ad-4ca7-42f3-8e94-c113a02ea63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TOKEN: Gus\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: Proto\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: is\n",
            "=====\n",
            "TAG: VBZ        POS: AUX\n",
            "EXPLANATION: verb, 3rd person singular present\n",
            "\n",
            "TOKEN: a\n",
            "=====\n",
            "TAG: DT         POS: DET\n",
            "EXPLANATION: determiner\n",
            "\n",
            "TOKEN: Python\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: developer\n",
            "=====\n",
            "TAG: NN         POS: NOUN\n",
            "EXPLANATION: noun, singular or mass\n",
            "\n",
            "TOKEN: currently\n",
            "=====\n",
            "TAG: RB         POS: ADV\n",
            "EXPLANATION: adverb\n",
            "\n",
            "TOKEN: working\n",
            "=====\n",
            "TAG: VBG        POS: VERB\n",
            "EXPLANATION: verb, gerund or present participle\n",
            "\n",
            "TOKEN: for\n",
            "=====\n",
            "TAG: IN         POS: ADP\n",
            "EXPLANATION: conjunction, subordinating or preposition\n",
            "\n",
            "TOKEN: a\n",
            "=====\n",
            "TAG: DT         POS: DET\n",
            "EXPLANATION: determiner\n",
            "\n",
            "TOKEN: London\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: -\n",
            "=====\n",
            "TAG: HYPH       POS: PUNCT\n",
            "EXPLANATION: punctuation mark, hyphen\n",
            "\n",
            "TOKEN: based\n",
            "=====\n",
            "TAG: VBN        POS: VERB\n",
            "EXPLANATION: verb, past participle\n",
            "\n",
            "TOKEN: Fintech\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: company\n",
            "=====\n",
            "TAG: NN         POS: NOUN\n",
            "EXPLANATION: noun, singular or mass\n",
            "\n",
            "TOKEN: .\n",
            "=====\n",
            "TAG: .          POS: PUNCT\n",
            "EXPLANATION: punctuation mark, sentence closer\n",
            "\n",
            "TOKEN: He\n",
            "=====\n",
            "TAG: PRP        POS: PRON\n",
            "EXPLANATION: pronoun, personal\n",
            "\n",
            "TOKEN: is\n",
            "=====\n",
            "TAG: VBZ        POS: AUX\n",
            "EXPLANATION: verb, 3rd person singular present\n",
            "\n",
            "TOKEN: interested\n",
            "=====\n",
            "TAG: JJ         POS: ADJ\n",
            "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
            "\n",
            "TOKEN: in\n",
            "=====\n",
            "TAG: IN         POS: ADP\n",
            "EXPLANATION: conjunction, subordinating or preposition\n",
            "\n",
            "TOKEN: learning\n",
            "=====\n",
            "TAG: VBG        POS: VERB\n",
            "EXPLANATION: verb, gerund or present participle\n",
            "\n",
            "TOKEN: Natural\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: Language\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: Processing\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: .\n",
            "=====\n",
            "TAG: .          POS: PUNCT\n",
            "EXPLANATION: punctuation mark, sentence closer\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = (\n",
        " \"Gus Proto is a Python developer currently\"\n",
        " \" working for a London-based Fintech\"\n",
        " \" company. He is interested in learning\"\n",
        " \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n",
        "for token in about_doc:\n",
        " print(\n",
        "     f\"\"\"\n",
        "TOKEN: {str(token)}\n",
        "=====\n",
        "TAG: {str(token.tag_):10} POS: {token.pos_}\n",
        "EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30dfd529",
      "metadata": {
        "id": "30dfd529",
        "outputId": "5b591b3a-06ab-427d-f864-51795798a167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[developer, company]\n",
            "[interested]\n"
          ]
        }
      ],
      "source": [
        "nouns = []\n",
        "adjectives = []\n",
        "for token in about_doc:\n",
        " if token.pos_ == \"NOUN\":\n",
        "     nouns.append(token)\n",
        " if token.pos_ == \"ADJ\":\n",
        "     adjectives.append(token)\n",
        "\n",
        "\n",
        "print(nouns)\n",
        "\n",
        "print(adjectives)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee700df",
      "metadata": {
        "id": "aee700df"
      },
      "source": [
        "### Preprocessing Functions:\n",
        "Examples: \n",
        "Lowercases the text\n",
        "Lemmatizes each token\n",
        "Removes punctuation symbols\n",
        "Removes stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef5df1c2",
      "metadata": {
        "id": "ef5df1c2",
        "outputId": "b58c14bc-6d89-4626-d655-7403d54f105d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['gus',\n",
              " 'proto',\n",
              " 'python',\n",
              " 'developer',\n",
              " 'currently',\n",
              " 'work',\n",
              " 'london',\n",
              " 'base',\n",
              " 'fintech',\n",
              " 'company',\n",
              " 'interested',\n",
              " 'learn',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'developer',\n",
              " 'conference',\n",
              " 'happen',\n",
              " '21',\n",
              " 'july',\n",
              " '2019',\n",
              " 'london',\n",
              " 'title',\n",
              " 'application',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'helpline',\n",
              " 'number',\n",
              " 'available',\n",
              " '+44',\n",
              " '1234567891',\n",
              " 'gus',\n",
              " 'helping',\n",
              " 'organize',\n",
              " 'keep',\n",
              " 'organize',\n",
              " 'local',\n",
              " 'python',\n",
              " 'meetup',\n",
              " 'internal',\n",
              " 'talk',\n",
              " 'workplace',\n",
              " 'gus',\n",
              " 'present',\n",
              " 'talk',\n",
              " 'talk',\n",
              " 'introduce',\n",
              " 'reader',\n",
              " 'use',\n",
              " 'case',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'fintech',\n",
              " 'apart',\n",
              " 'work',\n",
              " 'passionate',\n",
              " 'music',\n",
              " 'gus',\n",
              " 'learn',\n",
              " 'play',\n",
              " 'piano',\n",
              " 'enrol',\n",
              " 'weekend',\n",
              " 'batch',\n",
              " 'great',\n",
              " 'piano',\n",
              " 'academy',\n",
              " 'great',\n",
              " 'piano',\n",
              " 'academy',\n",
              " 'situate',\n",
              " 'mayfair',\n",
              " 'city',\n",
              " 'london',\n",
              " 'world',\n",
              " 'class',\n",
              " 'piano',\n",
              " 'instructor']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "complete_text = (\n",
        " \"Gus Proto is a Python developer currently\"\n",
        " \" working for a London-based Fintech company. He is\"\n",
        " \" interested in learning Natural Language Processing.\"\n",
        " \" There is a developer conference happening on 21 July\"\n",
        " ' 2019 in London. It is titled \"Applications of Natural'\n",
        " ' Language Processing\". There is a helpline number'\n",
        " \" available at +44-1234567891. Gus is helping organize it.\"\n",
        " \" He keeps organizing local Python meetups and several\"\n",
        " \" internal talks at his workplace. Gus is also presenting\"\n",
        " ' a talk. The talk will introduce the reader about \"Use'\n",
        " ' cases of Natural Language Processing in Fintech\".'\n",
        " \" Apart from his work, he is very passionate about music.\"\n",
        " \" Gus is learning to play the Piano. He has enrolled\"\n",
        " \" himself in the weekend batch of Great Piano Academy.\"\n",
        " \" Great Piano Academy is situated in Mayfair or the City\"\n",
        " \" of London and has world-class piano instructors.\"\n",
        ")\n",
        "complete_doc = nlp(complete_text)\n",
        "def is_token_allowed(token):\n",
        " return bool(\n",
        "     token\n",
        "     and str(token).strip()\n",
        "     and not token.is_stop\n",
        "     and not token.is_punct\n",
        " )\n",
        "\n",
        "def preprocess_token(token):\n",
        " return token.lemma_.strip().lower()\n",
        "\n",
        "complete_filtered_tokens = [\n",
        " preprocess_token(token)\n",
        " for token in complete_doc\n",
        " if is_token_allowed(token)\n",
        "]\n",
        "\n",
        "complete_filtered_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ccc16e",
      "metadata": {
        "id": "c6ccc16e"
      },
      "source": [
        "### Rule-Based Matching Using spaCy\n",
        "Rule-based matching is one of the steps in extracting information from unstructured text. It’s used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech).For example, with rule-based matching, you can extract a first name and a last name, which are always proper nouns:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de333643",
      "metadata": {
        "id": "de333643",
        "outputId": "c3915349-92be-4c9a-fed4-19af283047ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Gus Proto'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = (\n",
        " \"Gus Proto is a Python developer currently\"\n",
        " \" working for a London-based Fintech\"\n",
        " \" company. He is interested in learning\"\n",
        " \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def extract_full_name(nlp_doc):\n",
        " pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
        " matcher.add(\"FULL_NAME\", [pattern])\n",
        " matches = matcher(nlp_doc)\n",
        " for _, start, end in matches:\n",
        "     span = nlp_doc[start:end]\n",
        "     yield span.text\n",
        "\n",
        "\n",
        "next(extract_full_name(about_doc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd36b1a",
      "metadata": {
        "id": "bbd36b1a"
      },
      "source": [
        "### Named-Entity Recognition\n",
        "Named-entity recognition (NER) is the process of locating named entities in unstructured text and then classifying them into predefined categories, such as person names, organizations, locations, monetary values, percentages, and time expressions.\n",
        "\n",
        "You can use NER to learn more about the meaning of your text. For example, you could use it to populate tags for a set of documents in order to improve the keyword search. You could also use it to categorize customer support tickets into relevant categories.\n",
        "\n",
        "spaCy has the property .ents on Doc objects. You can use it to extract named entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20d837a",
      "metadata": {
        "id": "e20d837a",
        "outputId": "34485590-b150-4ff1-9505-8ba1c5ce331f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ent.text = 'Great Piano Academy'\n",
            "ent.start_char = 0\n",
            "ent.end_char = 19\n",
            "ent.label_ = 'ORG'\n",
            "spacy.explain('ORG') = Companies, agencies, institutions, etc.\n",
            "\n",
            "ent.text = 'Mayfair'\n",
            "ent.start_char = 35\n",
            "ent.end_char = 42\n",
            "ent.label_ = 'GPE'\n",
            "spacy.explain('GPE') = Countries, cities, states\n",
            "\n",
            "ent.text = 'the City of London'\n",
            "ent.start_char = 46\n",
            "ent.end_char = 64\n",
            "ent.label_ = 'GPE'\n",
            "spacy.explain('GPE') = Countries, cities, states\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "piano_class_text = (\n",
        " \"Great Piano Academy is situated\"\n",
        " \" in Mayfair or the City of London and has\"\n",
        " \" world-class piano instructors.\"\n",
        ")\n",
        "piano_class_doc = nlp(piano_class_text)\n",
        "\n",
        "for ent in piano_class_doc.ents:\n",
        " print(\n",
        "     f\"\"\"\n",
        "{ent.text = }\n",
        "{ent.start_char = }\n",
        "{ent.end_char = }\n",
        "{ent.label_ = }\n",
        "spacy.explain('{ent.label_}') = {spacy.explain(ent.label_)}\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9695e7c9",
      "metadata": {
        "id": "9695e7c9",
        "outputId": "04318576-5013-4ffb-b137-12be16b2749f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Out of 5 people surveyed, [REDACTED] , [REDACTED] and [REDACTED] like apples. [REDACTED] and [REDACTED] like oranges.\n"
          ]
        }
      ],
      "source": [
        "survey_text = (\n",
        " \"Out of 5 people surveyed, James Robert,\"\n",
        " \" Julie Fuller and Benjamin Brooks like\"\n",
        " \" apples. Kelly Cox and Matthew Evans\"\n",
        " \" like oranges.\"\n",
        ")\n",
        "\n",
        "\n",
        "def replace_person_names(token):\n",
        " if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
        "     return \"[REDACTED] \"\n",
        " return token.text_with_ws\n",
        "\n",
        "\n",
        "def redact_names(nlp_doc):\n",
        " with nlp_doc.retokenize() as retokenizer:\n",
        "     for ent in nlp_doc.ents:\n",
        "         retokenizer.merge(ent)\n",
        " tokens = map(replace_person_names, nlp_doc)\n",
        " return \"\".join(tokens)\n",
        "\n",
        "survey_doc = nlp(survey_text)\n",
        "print(redact_names(survey_doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b403c8",
      "metadata": {
        "id": "91b403c8",
        "outputId": "e33162cd-37b5-4448-e8e4-91e6837b2d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Paris', 'UK', 'Slovakia', 'Turkey', 'Syria'}\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"He left Syria and flew to Turkey, then went through Slovakia, Germanny and Paris and across to the UK.\")\n",
        "\n",
        "countries = set()\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == 'GPE':  # GPE stands for Geo-Political Entity\n",
        "        countries.add(ent.text)\n",
        "\n",
        "print(countries)  # {'Syria', 'Turkey', 'Slovakia', 'Germany', 'Paris', 'UK'}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322e5a25",
      "metadata": {
        "id": "322e5a25"
      },
      "source": [
        "### Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a3240b0",
      "metadata": {
        "id": "5a3240b0"
      },
      "source": [
        "Transformers are new types of recurrent neural networks that are particularly\n",
        "suitable for natural language processing. Transformers have become\n",
        "the state-of-\n",
        "the-\n",
        "art\n",
        "approach in natural language processing since 2017. With\n",
        "transformers you can build chatbot and question answering applications easily."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8275befc",
      "metadata": {
        "id": "8275befc"
      },
      "source": [
        "You might need to run the following commands in terminal to install transformers and tensorflow.\n",
        "\n",
        "- pip install transformers\n",
        "\n",
        "- pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7db1377",
      "metadata": {
        "id": "a7db1377",
        "outputId": "0bff7342-fcb5-48f8-e361-28ca3b76081d",
        "colab": {
          "referenced_widgets": [
            "b390c67cb6a84708ac6ea95f97cbf379",
            "02524a53caea484191dc2bb25245bda2",
            "76b419f24a7c4db69749fd2ed0238cc0"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b390c67cb6a84708ac6ea95f97cbf379",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02524a53caea484191dc2bb25245bda2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76b419f24a7c4db69749fd2ed0238cc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998718500137329}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "classifier('We are very happy to visit London.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21402c4f",
      "metadata": {
        "id": "21402c4f"
      },
      "source": [
        "### Q/A using transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97506bdb",
      "metadata": {
        "id": "97506bdb",
        "outputId": "5140d3cc-cd13-4136-84cf-32c731582cac",
        "colab": {
          "referenced_widgets": [
            "edf901bd92424e089b828ac6af2f0d3d",
            "c23721ffe9b34db5901aee7d23186f1f",
            "ece87fc4a64f43c6ba7cc914088fd089",
            "265bd46091a4421b96a8bbffbcf4e860",
            "c8bf49ea8a384f209e8406d884215aa0"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edf901bd92424e089b828ac6af2f0d3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c23721ffe9b34db5901aee7d23186f1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ece87fc4a64f43c6ba7cc914088fd089",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "265bd46091a4421b96a8bbffbcf4e860",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8bf49ea8a384f209e8406d884215aa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.7448018789291382,\n",
              " 'start': 11,\n",
              " 'end': 27,\n",
              " 'answer': 'Biox Systems Ltd'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example Q&A with transformers\n",
        "from transformers import pipeline\n",
        "question_answerer = pipeline('question-answering')\n",
        "question_answerer({'question': 'What is the name of the company?',\n",
        "                   'context': 'We created Biox Systems Ltd company back in the yearof 2000.'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e6d0cb",
      "metadata": {
        "id": "53e6d0cb",
        "outputId": "4582dc91-596c-443c-ff5e-7b3dcdd18282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question:How are you?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Biox Systems Ltd company\n",
            "Score: 0.26703327894210815\n"
          ]
        }
      ],
      "source": [
        "# Example open question answering with transformers\n",
        "from transformers import pipeline\n",
        "context = '''\n",
        "We created Biox Systems Ltd company back in the year of 2000.\n",
        "'''\n",
        "Question = input('Ask a question:')\n",
        "question_answerer = pipeline('question-answering')\n",
        "result = question_answerer(question=Question, context=context)\n",
        "print(\"Answer:\", result['answer'])\n",
        "print(\"Score:\", result['score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff322def",
      "metadata": {
        "id": "ff322def"
      },
      "source": [
        "Example shows another Python example on text generation by using Generative Pre-trained Transformer 2 (GPT-2)\n",
        "model Transformers. GPT-2 is a large transformer-based language model developed by OpenAI. The latest\n",
        "version is GPT-3. GPT-2 has 1.5 billion parameters and is trained on a dataset of 8 million web pages. GPT-2 \n",
        "is trained to predict the next word, given the previous words within the text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb1f281c",
      "metadata": {
        "id": "bb1f281c",
        "outputId": "78365284-cc41-4aca-f9ea-2fa2a437e64f",
        "colab": {
          "referenced_widgets": [
            "2c11d41f1b5c4e9f9112a72e8849df2b",
            "2656ef66ba6a461784754b9854971179",
            "03f1c3b7c82344329095a03311d7e676",
            "699522de159742998e621d7019a3948a",
            "c837daf2432343858d5f9170eeb3cd35",
            "c8e5fca06c924970882b2997b2bdac68"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c11d41f1b5c4e9f9112a72e8849df2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2656ef66ba6a461784754b9854971179",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03f1c3b7c82344329095a03311d7e676",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "699522de159742998e621d7019a3948a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c837daf2432343858d5f9170eeb3cd35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8e5fca06c924970882b2997b2bdac68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'I feel amazing about it.\"\\n\\nAnd she was already working on that.'},\n",
              " {'generated_text': 'I feel amazing about this.\"\\n\\nHe said he was shocked that the school would take the case'},\n",
              " {'generated_text': 'I feel amazing about that. We started it by calling a party. At home to work on it'},\n",
              " {'generated_text': 'I feel amazing about where the team should take the offseason. It\\'s going to be great,\" Williams'},\n",
              " {'generated_text': 'I feel amazing about this project. The idea of this album being included in the album is just so'}]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example Text Generation with transformers (GPT-2 Model)\n",
        "# pip install transformers\n",
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation',\n",
        "model='gpt2')\n",
        "set_seed(20)\n",
        "generator(\"I feel amazing about\", max_length=20,\n",
        "num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34001844",
      "metadata": {
        "id": "34001844"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}